{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Project: Theory Discourse Analysis\n",
        "\n",
        "This notebook prototypes early end-to-end pipeline steps for a theory case study:\n",
        "retrieval, availability checks, and paragraph selection using embeddings.\n",
        "\n",
        "Outputs:\n",
        "- Intermediate lists (e.g., unavailable DOIs)\n",
        "- Final merged CSV containing selected paragraphs + similarity scores\n",
        "\n",
        "Notes:\n",
        "- Retrieval steps should rely on lawful access routes (publisher/library access, OA APIs, manual downloads).\n",
        "- Embedding-based paragraph selection is heuristic and should be validated.\n",
        "- Requires an API key in the environment (e.g., GPT4_KEY).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CONFIG\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "# Base directory for inputs/outputs (edit to your setup)\n",
        "DATA_DIR = Path(\"../data\")\n",
        "\n",
        "# Inputs\n",
        "DOI_CSV = DATA_DIR / \"crossref_relevant_dois.csv\"\n",
        "EBSCO_XML = DATA_DIR / \"ebsco_articles.xml\"\n",
        "XML_DIR = DATA_DIR / \"articles_xml_final\"\n",
        "INPUT_CSV = DATA_DIR / \"pre_result.csv\"\n",
        "\n",
        "# Outputs\n",
        "OUT_CSV = DATA_DIR / \"final.csv\"\n",
        "\n",
        "# Paragraph selection\n",
        "TOP_N_PARAGRAPHS = 3\n",
        "\n",
        "# Embeddings\n",
        "EMBED_MODEL = \"text-embedding-ada-002\"  # keep consistent with your existing pipeline\n",
        "\n",
        "# How many articles to process (for quick tests)\n",
        "MAX_ARTICLES = None  # set to an int for debugging, e.g., 5\n",
        "\n",
        "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "print(\"DATA_DIR:\", DATA_DIR.resolve())\n",
        "print(\"OUT_CSV:\", OUT_CSV.resolve())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports for retrieval + helpers\n",
        "\n",
        "import csv\n",
        "import os\n",
        "import xml.etree.ElementTree as ET\n",
        "import pandas as pd\n",
        "\n",
        "from fetch_articles_crossref import fetch_articles_crossref\n",
        "from fetch_articles_ebsco import fetch_articles_ebsco\n",
        "\n",
        "# NOTE:\n",
        "# This public repository does not include automated downloading of copyrighted PDFs.\n",
        "# If you need PDF acquisition, implement it via lawful access routes in a dedicated script\n",
        "# (publisher/library links, OA sources, institutional access, or manual downloads).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Read DOI data from file and (optionally) fetch via Crossref\n",
        "\n",
        "if DOI_CSV.exists():\n",
        "    with DOI_CSV.open(newline=\"\") as f:\n",
        "        reader = csv.reader(f)\n",
        "        row = list(reader)\n",
        "        dois = row[0][1:] if row and len(row[0]) > 1 else []\n",
        "    print(\"# DOIs:\", len(dois))\n",
        "    # fetch_articles_crossref(dois)  # uncomment if you want to run retrieval\n",
        "else:\n",
        "    print(\"DOI_CSV not found:\", DOI_CSV)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fetch available articles from EBSCO (interactive / institution-specific workflow)\n",
        "\n",
        "# fetch_articles_ebsco()  # uncomment if you want to run the Selenium workflow\n",
        "print(\"EBSCO fetch is disabled by default in this notebook.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helper: retrieve DOIs of unavailable EBSCO articles based on an XML export\n",
        "\n",
        "def ebsco_get_unavailable_article_dois(filepath: Path):\n",
        "    tree = ET.parse(str(filepath))\n",
        "    root = tree.getroot()\n",
        "\n",
        "    unavailable_article_dois = []\n",
        "    for article in root.findall(\"rec\"):\n",
        "        formats = article.find(\".//header/controlInfo/artinfo/formats\")\n",
        "        if formats is None:\n",
        "            doi = article.find(\".//ui[@type='doi']\")\n",
        "            if doi is not None and doi.text:\n",
        "                unavailable_article_dois.append(doi.text)\n",
        "    return unavailable_article_dois\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check which articles are not available to download from EBSCO according to the XML overview file\n",
        "\n",
        "if EBSCO_XML.exists():\n",
        "    ebsco_unavailable_article_dois = ebsco_get_unavailable_article_dois(EBSCO_XML)\n",
        "    print(\"Articles not retrievable via EBSCO UI (per XML):\", len(ebsco_unavailable_article_dois))\n",
        "    print(ebsco_unavailable_article_dois[:20], \"...\" if len(ebsco_unavailable_article_dois) > 20 else \"\")\n",
        "else:\n",
        "    print(\"EBSCO_XML not found:\", EBSCO_XML)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Paragraph selection via embeddings\n",
        "\n",
        "Given TEI XML article files, extract paragraphs, embed them, and select the most relevant ones for a theory query.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Embedding + similarity helpers\n",
        "\n",
        "import numpy as np\n",
        "import openai\n",
        "from dotenv import load_dotenv\n",
        "from openai.embeddings_utils import get_embedding, cosine_similarity\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "load_dotenv()\n",
        "openai.api_key = os.getenv(\"GPT4_KEY\")\n",
        "\n",
        "if not openai.api_key:\n",
        "    print(\"WARNING: GPT4_KEY not found in environment. Embeddings will fail until you set it.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def search_paragraphs(df: pd.DataFrame, query: str, n: int = 5):\n",
        "    query_embedding = get_embedding(query, engine=EMBED_MODEL)\n",
        "    df[\"similarity\"] = df[\"embeddings\"].apply(lambda x: cosine_similarity(x, query_embedding))\n",
        "\n",
        "    results = (\n",
        "        df.sort_values(\"similarity\", ascending=False)\n",
        "          .head(n)\n",
        "          .sort_index()\n",
        "    )\n",
        "    return df, results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the input CSV (must include a 'filename' column pointing to TEI XML files)\n",
        "\n",
        "if not INPUT_CSV.exists():\n",
        "    raise FileNotFoundError(f\"INPUT_CSV not found: {INPUT_CSV}\")\n",
        "if not XML_DIR.exists():\n",
        "    raise FileNotFoundError(f\"XML_DIR not found: {XML_DIR}\")\n",
        "\n",
        "df_in = pd.read_csv(INPUT_CSV)\n",
        "print(\"Rows in input:\", len(df_in))\n",
        "\n",
        "if MAX_ARTICLES is not None:\n",
        "    df_in = df_in.head(int(MAX_ARTICLES)).copy()\n",
        "    print(\"Limiting to MAX_ARTICLES:\", len(df_in))\n",
        "\n",
        "dfs = []\n",
        "\n",
        "for _, row in df_in.iterrows():\n",
        "    filename = row[\"filename\"]\n",
        "    xml_path = XML_DIR / filename\n",
        "    if not xml_path.exists():\n",
        "        continue\n",
        "\n",
        "    tree = ET.parse(str(xml_path))\n",
        "    root = tree.getroot()\n",
        "\n",
        "    paragraphs = root.findall(\".//{http://www.tei-c.org/ns/1.0}p\")\n",
        "    paragraphs = [\"\".join(p.itertext()).strip() for p in paragraphs]\n",
        "    paragraphs = [p for p in paragraphs if p]\n",
        "\n",
        "    if paragraphs:\n",
        "        df = pd.DataFrame({\"filename\": filename, \"paragraphs\": paragraphs})\n",
        "        dfs.append(df)\n",
        "\n",
        "print(\"Articles with paragraphs:\", len(dfs))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert paragraphs into embeddings\n",
        "\n",
        "for df in dfs:\n",
        "    df[\"embeddings\"] = df[\"paragraphs\"].apply(lambda t: get_embedding(t, engine=EMBED_MODEL))\n",
        "    print(\"Embedded:\", df[\"filename\"].iloc[0], \"#paragraphs:\", len(df))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select TOP_N_PARAGRAPHS most relevant paragraphs per article\n",
        "\n",
        "query = \"\"\"The concept of memory decay in scientific psychology describes that\n",
        "memory traces are stored with an initial strength value and that this strength decays passively over time unless it is reactivated.\n",
        "Reactivation of memory traces according to the memory decay theory can be done by practice.\n",
        "Once the activation level for a stored memory trace becomes too low, the memory trace is lost.\n",
        "The memory decay theory concerns memory loss in healthy individuals.\n",
        "Changes solely due to aging processes and abnormal changes in memory capacity due to impairments like dementia are not the explanatory focus of this theory.\"\"\"\n",
        "\n",
        "selected = []\n",
        "for df in dfs:\n",
        "    _, res = search_paragraphs(df, query, n=TOP_N_PARAGRAPHS)\n",
        "    res = res.reset_index(drop=True)\n",
        "    selected.append(res)\n",
        "\n",
        "print(\"Selected paragraph sets:\", len(selected))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Merge selections back into a single CSV (p1â€“p3 + embeddings + cosine similarities)\n",
        "\n",
        "final_rows = []\n",
        "\n",
        "for res in selected:\n",
        "    if len(res) < TOP_N_PARAGRAPHS:\n",
        "        continue\n",
        "\n",
        "    filename = res.loc[0, \"filename\"]\n",
        "\n",
        "    row = {\n",
        "        \"filename\": filename,\n",
        "        \"p1\": res.loc[0, \"paragraphs\"],\n",
        "        \"p1_embedding\": res.loc[0, \"embeddings\"],\n",
        "        \"p1_cos_similarity\": res.loc[0, \"similarity\"],\n",
        "        \"p1_rating_category\": \"\",\n",
        "        \"p1_rating_rationale\": \"\",\n",
        "        \"p2\": res.loc[1, \"paragraphs\"],\n",
        "        \"p2_embedding\": res.loc[1, \"embeddings\"],\n",
        "        \"p2_cos_similarity\": res.loc[1, \"similarity\"],\n",
        "        \"p2_rating_category\": \"\",\n",
        "        \"p2_rating_rationale\": \"\",\n",
        "        \"p3\": res.loc[2, \"paragraphs\"],\n",
        "        \"p3_embedding\": res.loc[2, \"embeddings\"],\n",
        "        \"p3_cos_similarity\": res.loc[2, \"similarity\"],\n",
        "        \"p3_rating_category\": \"\",\n",
        "        \"p3_rating_rationale\": \"\",\n",
        "    }\n",
        "    final_rows.append(row)\n",
        "\n",
        "df_out = pd.DataFrame(final_rows)\n",
        "\n",
        "# Optional: merge with df_in if you want to keep original metadata columns\n",
        "df_merged = pd.merge(df_in, df_out, on=\"filename\", how=\"left\")\n",
        "\n",
        "df_merged.to_csv(OUT_CSV, index=False)\n",
        "print(\"Wrote:\", OUT_CSV)\n",
        "print(\"Rows:\", len(df_merged))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.12"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
